#!/usr/bin/env python3
"""
è¯¦ç»†åˆ†æžå„ä¸ªé¢†åŸŸçš„Rewardè®¡ç®—è¿‡ç¨‹å’Œç»“æžœ
"""
import sys
sys.path.append('/root/work/Agentic-RL-Scaling-Law')

import pandas as pd
import json
from src.reward.guru_reward_improved import compute_score, extract_answer, normalize_answer
from pathlib import Path

def analyze_domain_rewards():
    
    domains = [
        ('math', 'math__combined_54.4k.parquet'),
        ('logic', 'logic__arcagi1_111.parquet'), 
        ('code', 'codegen__leetcode2k_1.3k.parquet'),
        ('stem', 'stem__web_3.6k.parquet')
    ]
    
    base_dir = Path('../../data/guru_verl/train')
    
    print("="*120)

    for domain_name, filename in domains:
        filepath = base_dir / filename
        if not filepath.exists():
            print(f"\nâŒ {domain_name.upper()} - æ–‡ä»¶ä¸å­˜åœ¨: {filename}")
            continue
            
        print(f"\n{'ðŸ§®' if domain_name=='math' else 'ðŸ’»' if domain_name=='code' else 'ðŸ§©' if domain_name=='logic' else 'ðŸ”¬'} {domain_name.upper()} é¢†åŸŸåˆ†æž")
        print("="*80)
        
        df = pd.read_parquet(filepath)
        print(f"ðŸ“Š æ•°æ®é›†å¤§å°: {len(df)} æ ·æœ¬")
        
        # é€‰æ‹©2-3ä¸ªæœ‰ä»£è¡¨æ€§çš„æ ·æœ¬è¿›è¡Œè¯¦ç»†åˆ†æž
        sample_indices = [0, len(df)//4, len(df)//2] if len(df) > 3 else list(range(len(df)))
        
        for i, idx in enumerate(sample_indices[:3]):  # æœ€å¤šåˆ†æž3ä¸ªæ ·æœ¬
            print(f"\n--- æ ·æœ¬ {i+1} (ç´¢å¼•: {idx}) ---")
            sample = df.iloc[idx]
            
            # 1. æ˜¾ç¤ºé—®é¢˜
            if 'prompt' in sample and len(sample['prompt']) > 0:
                problem = sample['prompt'][0]['content']
                print(f"ðŸ“ é—®é¢˜: {problem[:200]}...")
            
            # 2. æ˜¾ç¤ºground truth
            ground_truth = sample['reward_model']['ground_truth']
            print(f"ðŸŽ¯ Ground Truthç±»åž‹: {type(ground_truth)}")
            
            if domain_name == 'code' and isinstance(ground_truth, str):
                try:
                    gt_dict = json.loads(ground_truth)
                    if 'functional' in gt_dict:
                        print(f"ðŸ§ª æµ‹è¯•å‡½æ•°:\n{gt_dict['functional'][:300]}...")
                except:
                    print(f"ðŸŽ¯ Ground Truth: {str(ground_truth)[:200]}...")
            else:
                if len(str(ground_truth)) > 200:
                    print(f"ðŸŽ¯ Ground Truth: {str(ground_truth)[:200]}...")
                else:
                    print(f"ðŸŽ¯ Ground Truth: {ground_truth}")
            
            # 3. æ˜¾ç¤ºæ•°æ®é›†ä¸­çš„ç­”æ¡ˆ
            # çŽ°åœ¨æ‰€æœ‰é¢†åŸŸçš„ground truthéƒ½åœ¨reward_modelä¸­
            if 'ground_truth' in sample['reward_model']:
                dataset_answer = sample['reward_model']['ground_truth']
                
                if domain_name == 'code':
                    # Codeé¢†åŸŸçš„ground_truthæ˜¯æµ‹è¯•ä»£ç ï¼Œåˆ›å»ºä¸€ä¸ªç®€å•çš„Solutionä½œä¸ºç­”æ¡ˆ
                    # è¿™é‡Œåªæ˜¯ä¸ºäº†æ¼”ç¤ºï¼Œå®žé™…è®­ç»ƒæ—¶ä¼šç”ŸæˆçœŸå®žçš„ä»£ç 
                    formatted_solution = """```python
class Solution:
    def solve(self):
        # This is a placeholder solution for demonstration
        return None
```"""
                    print(f"ðŸ’¡ æ¼”ç¤ºç­”æ¡ˆ: [ä½¿ç”¨å ä½ç¬¦ä»£ç ]")
                else:
                    formatted_solution = dataset_answer
                    if len(str(dataset_answer)) > 200:
                        print(f"ðŸ’¡ æ•°æ®é›†çœŸå®žç­”æ¡ˆ: {str(dataset_answer)[:200]}...")
                    else:
                        print(f"ðŸ’¡ æ•°æ®é›†çœŸå®žç­”æ¡ˆ: {dataset_answer}")
            else:
                # åˆ›å»ºä¸€ä¸ªæ¨¡æ‹Ÿçš„é”™è¯¯ç­”æ¡ˆè¿›è¡Œæµ‹è¯•
                dataset_answer = "This is a test wrong answer"
                formatted_solution = dataset_answer
                print(f"ðŸ’¡ æ¨¡æ‹Ÿæµ‹è¯•ç­”æ¡ˆ: {dataset_answer}")
            
            # 4. è¯¦ç»†è®¡ç®—è¿‡ç¨‹
            print(f"\nðŸ” Rewardè®¡ç®—è¿‡ç¨‹:")
            
            # 4.1 ç­”æ¡ˆæå–
            extracted = extract_answer(formatted_solution, domain_name)
            print(f"   æ­¥éª¤1 - ç­”æ¡ˆæå–:")
            if len(str(extracted)) > 150:
                print(f"     æå–ç»“æžœ: {str(extracted)[:150]}...")
            else:
                print(f"     æå–ç»“æžœ: {extracted}")
            
            # 4.2 ç­”æ¡ˆæ ‡å‡†åŒ–
            if domain_name != 'code':  # code domainä¸éœ€è¦æ ‡å‡†åŒ–æ¯”è¾ƒ
                normalized_solution = normalize_answer(extracted, domain_name)
                normalized_truth = normalize_answer(ground_truth, domain_name)
                print(f"   æ­¥éª¤2 - ç­”æ¡ˆæ ‡å‡†åŒ–:")
                print(f"     æ ‡å‡†åŒ–è§£ç­”: {normalized_solution}")
                print(f"     æ ‡å‡†åŒ–çœŸå€¼: {normalized_truth}")
            
            # 4.3 æœ€ç»ˆè¯„åˆ†
            try:
                score = compute_score(
                    formatted_solution,
                    ground_truth,
                    data_source=sample.get('data_source', ''),
                    domain=domain_name
                )
                print(f"   æ­¥éª¤3 - æœ€ç»ˆè¯„åˆ†: {score:.4f}")
                
                # è§£é‡Šè¯„åˆ†ç»“æžœ
                if domain_name == 'math':
                    if score >= 0.99:
                        print(f"     âœ… æ•°å­¦ç­”æ¡ˆæ­£ç¡® (ä½¿ç”¨VeRL math_scoreæˆ–ç²¾ç¡®åŒ¹é…)")
                    else:
                        print(f"     âŒ æ•°å­¦ç­”æ¡ˆé”™è¯¯")
                        
                elif domain_name == 'code':
                    if score >= 0.99:
                        print(f"     âœ… ä»£ç é€šè¿‡æ‰€æœ‰æµ‹è¯•ç”¨ä¾‹")
                    elif score > 0:
                        total_tests = len([line for line in str(ground_truth) if 'assert' in line])
                        passed_tests = int(score * total_tests)
                        print(f"     ðŸ”¶ ä»£ç éƒ¨åˆ†æ­£ç¡®: {passed_tests}/{total_tests} æµ‹è¯•ç”¨ä¾‹é€šè¿‡")
                    else:
                        print(f"     âŒ ä»£ç é”™è¯¯ (è¯­æ³•é”™è¯¯ã€ç»“æž„é”™è¯¯æˆ–æ‰€æœ‰æµ‹è¯•å¤±è´¥)")
                        
                elif domain_name == 'logic':
                    if score >= 0.99:
                        print(f"     âœ… é€»è¾‘ç­”æ¡ˆå®Œå…¨åŒ¹é…")
                    else:
                        print(f"     âŒ é€»è¾‘ç­”æ¡ˆä¸åŒ¹é…")
                        
                elif domain_name == 'stem':
                    if score >= 0.99:
                        print(f"     âœ… ç§‘å­¦ç­”æ¡ˆæ­£ç¡® (æ•°å­¦è¯„åˆ†æˆ–ç²¾ç¡®åŒ¹é…)")
                    else:
                        print(f"     âŒ ç§‘å­¦ç­”æ¡ˆé”™è¯¯")
                
            except Exception as e:
                print(f"   âŒ è¯„åˆ†è®¡ç®—å‡ºé”™: {e}")
                import traceback
                traceback.print_exc()
        
        # ç»Ÿè®¡è¯¥é¢†åŸŸçš„æ•´ä½“è¡¨çŽ°
        print(f"\nðŸ“ˆ {domain_name.upper()} é¢†åŸŸæ•´ä½“ç»Ÿè®¡ (å‰50ä¸ªæ ·æœ¬):")
        test_count = min(50, len(df))
        scores = []
        
        for idx in range(test_count):
            try:
                sample = df.iloc[idx]
                ground_truth = sample['reward_model']['ground_truth']
                
                if domain_name == 'code':
                    # Codeé¢†åŸŸä½¿ç”¨å ä½ç¬¦ä»£ç æµ‹è¯•ï¼ˆå®žé™…ä¼šæ˜¯0åˆ†ï¼‰
                    solution = """```python
class Solution:
    def solve(self):
        return None
```"""
                else:
                    # å¯¹äºŽå…¶ä»–é¢†åŸŸï¼Œæˆ‘ä»¬ç”¨ground_truthä½œä¸º"æ­£ç¡®ç­”æ¡ˆ"æ¥æµ‹è¯•
                    # è¿™æ ·åº”è¯¥å¾—åˆ°é«˜åˆ†ï¼ŒéªŒè¯rewardå‡½æ•°å·¥ä½œæ­£å¸¸
                    solution = ground_truth
                
                score = compute_score(
                    solution,
                    ground_truth,
                    data_source=sample.get('data_source', ''),
                    domain=domain_name
                )
                scores.append(score)
            except Exception as e:
                # print(f"Error processing sample {idx}: {e}")
                pass
        
        if scores:
            avg_score = sum(scores) / len(scores)
            perfect_count = sum(1 for s in scores if s >= 0.99)
            partial_count = sum(1 for s in scores if 0.1 <= s < 0.99)
            zero_count = sum(1 for s in scores if s < 0.1)
            
            print(f"   æœ‰æ•ˆæ ·æœ¬æ•°: {len(scores)}/{test_count}")
            print(f"   å¹³å‡å¾—åˆ†: {avg_score:.4f}")
            print(f"   å®Œç¾Žå¾—åˆ†(â‰¥0.99): {perfect_count} ({perfect_count/len(scores)*100:.1f}%)")
            print(f"   éƒ¨åˆ†å¾—åˆ†(0.1-0.99): {partial_count} ({partial_count/len(scores)*100:.1f}%)")
            print(f"   é›¶åˆ†æˆ–æŽ¥è¿‘é›¶åˆ†(<0.1): {zero_count} ({zero_count/len(scores)*100:.1f}%)")

def explain_reward_mechanisms():
    """è§£é‡Šå„é¢†åŸŸçš„rewardè®¡ç®—æœºåˆ¶"""
    
    print(f"\n{'='*120}")
    print("å„é¢†åŸŸRewardè®¡ç®—æœºåˆ¶åŽŸç†è¯´æ˜Ž")
    print("="*120)
    
    mechanisms = {
        "ðŸ§® MATHé¢†åŸŸ": {
            "è®¡ç®—æ–¹å¼": [
                "1. ä¼˜å…ˆä½¿ç”¨VeRLå†…ç½®çš„math_score()å‡½æ•°è¿›è¡Œæ•°å­¦è¡¨è¾¾å¼è¯„ä¼°",
                "2. è‡ªåŠ¨è¯†åˆ«\\boxed{ç­”æ¡ˆ}æ ¼å¼å’Œ'final answer'ã€'answer is'ç­‰æ¨¡å¼",
                "3. å¦‚æžœmath_score()å¤±è´¥ï¼Œå›žé€€åˆ°æ ‡å‡†åŒ–å­—ç¬¦ä¸²æ¯”è¾ƒ",
                "4. æ”¯æŒæ•°å­¦ç¬¦å·ã€åˆ†æ•°ã€æ–¹ç¨‹å¼ç­‰å¤æ‚æ ¼å¼"
            ],
            "è¯„åˆ†è§„åˆ™": "äºŒå…ƒè¯„åˆ†: 1.0(æ­£ç¡®) æˆ– 0.0(é”™è¯¯)",
            "ä¼˜åŠ¿": "ä¸“é—¨é’ˆå¯¹æ•°å­¦é—®é¢˜ä¼˜åŒ–ï¼Œèƒ½å¤„ç†å„ç§æ•°å­¦è¡¨è¾¾å¼æ ¼å¼"
        },
        
        "ðŸ’» CODEé¢†åŸŸ": {
            "è®¡ç®—æ–¹å¼": [
                "1. ä»Žmarkdownä»£ç å—ä¸­æå–Pythonä»£ç ",
                "2. éªŒè¯ä»£ç ç»“æž„(å¿…é¡»åŒ…å«class Solutionå’Œæ–¹æ³•å®šä¹‰)",
                "3. åœ¨å®‰å…¨çš„å—é™çŽ¯å¢ƒä¸­æ‰§è¡Œä»£ç ",
                "4. è§£æžJSONæ ¼å¼çš„å•å…ƒæµ‹è¯•å¹¶é€ä¸ªæ‰§è¡Œ",
                "5. è®¡ç®—é€šè¿‡çš„æµ‹è¯•ç”¨ä¾‹æ¯”ä¾‹ä½œä¸ºåˆ†æ•°"
            ],
            "è¯„åˆ†è§„åˆ™": "æ¢¯åº¦è¯„åˆ†: 0.0-1.0 (é€šè¿‡çš„æµ‹è¯•ç”¨ä¾‹æ¯”ä¾‹)",
            "ä¼˜åŠ¿": "å®žé™…æ‰§è¡Œä»£ç å’Œæµ‹è¯•ï¼Œæä¾›ç²¾ç¡®çš„åŠŸèƒ½æ­£ç¡®æ€§è¯„ä¼°"
        },
        
        "ðŸ§© LOGICé¢†åŸŸ": {
            "è®¡ç®—æ–¹å¼": [
                "1. ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼åŒ¹é…ç»“æž„åŒ–ç­”æ¡ˆæ¨¡å¼",
                "2. è¯†åˆ«'answer:'ã€'therefore'ã€ç»“è®ºç­‰å…³é”®è¯",
                "3. æ ‡å‡†åŒ–å¸ƒå°”å€¼ç­”æ¡ˆ(Yes/No, True/Falseç­‰)",
                "4. è¿›è¡Œç²¾ç¡®å­—ç¬¦ä¸²åŒ¹é…æ¯”è¾ƒ"
            ],
            "è¯„åˆ†è§„åˆ™": "äºŒå…ƒè¯„åˆ†: 1.0(å®Œå…¨åŒ¹é…) æˆ– 0.0(ä¸åŒ¹é…)",
            "ä¼˜åŠ¿": "é’ˆå¯¹é€»è¾‘æŽ¨ç†é—®é¢˜çš„ç»“æž„åŒ–ç­”æ¡ˆæ ¼å¼ä¼˜åŒ–"
        },
        
        "ðŸ”¬ STEM/SCIENCEé¢†åŸŸ": {
            "è®¡ç®—æ–¹å¼": [
                "1. é¦–å…ˆå°è¯•ä½¿ç”¨math_score()å¤„ç†æ•°å€¼åž‹ç§‘å­¦é—®é¢˜",
                "2. æ”¯æŒç§‘å­¦è®¡æ•°æ³•ã€ç‰©ç†å…¬å¼ç­‰æ ¼å¼",
                "3. å¦‚æžœæ•°å­¦è¯„åˆ†å¤±è´¥ï¼Œå›žé€€åˆ°ç²¾ç¡®å­—ç¬¦ä¸²åŒ¹é…",
                "4. å¤„ç†æ–¹ç¨‹å¼ã€ç‰©ç†é‡ã€åŒ–å­¦å¼ç­‰ç§‘å­¦è¡¨è¾¾å¼"
            ],
            "è¯„åˆ†è§„åˆ™": "äºŒå…ƒè¯„åˆ†: 1.0(æ­£ç¡®) æˆ– 0.0(é”™è¯¯)",
            "ä¼˜åŠ¿": "ç»“åˆæ•°å­¦è¯„åˆ†å’Œç²¾ç¡®åŒ¹é…ï¼Œé€‚åº”ç§‘å­¦é—®é¢˜çš„å¤šæ ·æ€§"
        }
    }
    
    for domain, info in mechanisms.items():
        print(f"\n{domain}")
        print("-" * 60)
        print("ðŸ”§ è®¡ç®—æ–¹å¼:")
        for step in info["è®¡ç®—æ–¹å¼"]:
            print(f"   {step}")
        print(f"ðŸ“Š è¯„åˆ†è§„åˆ™: {info['è¯„åˆ†è§„åˆ™']}")
        print(f"âœ¨ ä¼˜åŠ¿: {info['ä¼˜åŠ¿']}")

if __name__ == "__main__":
    analyze_domain_rewards()
    explain_reward_mechanisms()
    
    print(f"\n{'='*120}")
    print("å…³é”®æ”¹è¿›æ€»ç»“")
    print("="*120)
    print("""
ðŸš€ ä¸»è¦æ”¹è¿›:
1. CODEé¢†åŸŸ: ä»Žå®Œå…¨æ— æ•ˆ(æ€»æ˜¯0åˆ†) â†’ å®žé™…æ‰§è¡Œå•å…ƒæµ‹è¯•çš„æ¢¯åº¦è¯„åˆ†ç³»ç»Ÿ
2. å®‰å…¨æ‰§è¡Œ: åœ¨å—é™çŽ¯å¢ƒä¸­å®‰å…¨æ‰§è¡Œç”¨æˆ·ä»£ç ï¼Œé˜²æ­¢æ¶æ„æ“ä½œ
3. éƒ¨åˆ†æ­£ç¡®æ”¯æŒ: CODEé¢†åŸŸæ”¯æŒ0.0-1.0çš„è¿žç»­è¯„åˆ†
4. é²æ£’æ€§å¢žå¼º: æ›´å¥½çš„é”™è¯¯å¤„ç†å’Œå›žé€€æœºåˆ¶
5. ç­”æ¡ˆæå–ä¼˜åŒ–: æ”¹è¿›å„é¢†åŸŸçš„ç­”æ¡ˆæ¨¡å¼è¯†åˆ«

ðŸ“ˆ å½±å“:
- æå‡PPOè®­ç»ƒä¸­CODEé¢†åŸŸçš„rewardä¿¡å·è´¨é‡
- ä¸ºéƒ¨åˆ†æ­£ç¡®çš„è§£ç­”æä¾›åˆç†çš„æ¢¯åº¦åé¦ˆ
- å¢žå¼ºæ•´ä½“è®­ç»ƒç¨³å®šæ€§å’Œæ”¶æ•›æ•ˆæžœ
""")