#!/usr/bin/env python3
"""
Scaling Law Pipeline - Multi-Metric Analysis
Processes multiple test metrics from Experiment1 data and generates scaling law plots for each metric
"""

import os
import sys
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from pathlib import Path
from scipy.optimize import curve_fit

# Add current directory to path for imports
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

import intrinsic
import data_proc
import plot

# =============================================================================
# CONFIGURATION - Edit these variables to customize the run
# =============================================================================

# Data source configuration - use absolute paths based on script location
SCRIPT_DIR = Path(__file__).parent
OUTPUT_BASE_DIR = SCRIPT_DIR / "outputs"  # Base output directory for PNG plots
SAMPLE_SIZE_PER_STEP = 512
BUILD_I_ON_SMOOTHED = True
WARMUP_CLIPPING_NUM = 20


PLOT_BASIC_CURVES = False # True for Intrinsic Curves

HOLDOUT=True
if HOLDOUT:
    # Test metrics to process (from the CSV columns)
    TEST_METRICS = [
        'holdout_score',
    ]
    FIGURE_PREFIX = 'holdout'
    FIGURE_COLUMNS = 1 # note: if total > figure_columns, [row, col] -> [i]
    FIGURE_SIZE=(5, 5)
else:
    # Test metrics to process (from the CSV columns)
    TEST_METRICS = [
        'overall_pass1', 
        'val/test_score/openai/gsm8k',
        'val/test_score/codegen__humaneval',
        'val/test_score/stem__supergpqa',
        'val/test_score/math__math',
        'val/test_score/logic__zebra_puzzle_dataset',
        'val/test_score/aimeamc2023',
        'val/test_score/aime2024',
        # 'holdout_score',
        # 'val/test_score/math__deepscaler_preview',
        # 'val/test_score/math__merged_deduped_dapo_or1_dataset',
    ]
    # FIGURE_PREFIX = 'holdout'
    FIGURE_PREFIX = 'all'
    FIGURE_COLUMNS = 2 # note: if total > figure_columns, [row, col] -> [i]
    FIGURE_SIZE=(10, 10)


total_metrics = len(TEST_METRICS)
phi_global = 1.0

DEBUG = False  # Set to False to disable data statistics printing

# =============================================================================
# MAIN PROCESSING FUNCTION
# =============================================================================
def main():
    """Main processing function"""
    global phi_global
    

    csv_exp1_run0 = SCRIPT_DIR / "csv" / "scaling_law_data_experiment1_instruct_run0.csv" 
    csv_exp1_run1 = SCRIPT_DIR / "csv" / "scaling_law_data_experiment1_instruct_run1.csv" 
    csv_exp1_run2 = SCRIPT_DIR / "csv" / "scaling_law_data_experiment1_instruct_run2.csv" 

    print("=== Multi-Metric Scaling Law Analysis ===")
    print(f"Processing CSV: {csv_exp1_run0} and {csv_exp1_run1}")
    print(f"Output directory: {OUTPUT_BASE_DIR}")
    print(f"Metrics to process: {len(TEST_METRICS)}")
    
    # Load data
    if not csv_exp1_run0.exists():
        print(f"âŒ CSV file not found: {csv_exp1_run0}")
        return
    if not csv_exp1_run1.exists():
        print(f"âŒ CSV file not found: {csv_exp1_run1}")
        return
    
    df_run0 = pd.read_csv(csv_exp1_run0)
    df_run1 = pd.read_csv(csv_exp1_run1)
    df_run2 = pd.read_csv(csv_exp1_run2)

    df = pd.concat([df_run0, df_run1, df_run2], ignore_index=True)
    # df = df_run2
    print (df.columns)
    
    # Sort, Inspect, Validate and Normalize data
    df = df.sort_values(['model_size','runid','step']).reset_index(drop=True)
    # data_proc.inspect_data(df)
    data_proc.validate_data(df, metric_columns=TEST_METRICS)
    df = data_proc.normalize_data(df)
    # Calculate E = step * sample_size_per_step
    df['E'] = df['step'] * float(SAMPLE_SIZE_PER_STEP)

    # Estimate global efficiency parameter phi
    phi_global, phi_by_N, phi_stats_df = data_proc.estimate_phi_from_runs(
        df, 
        sample_size_per_step=SAMPLE_SIZE_PER_STEP, 
        tail_fraction=0.5
    )
    print(f"phi (global tail median) = {phi_global}")
    
    # Recalculate C using the estimated phi_global
    df['C'] = df['N'] * df['E'] * phi_global
    
    # # Print data statistics if enabled
    # if DEBUG:
    #     print("\n" + "="*50)
    #     print("Raw data statistics:")
    #     data_proc.print_data_statistics(df_merged)
    
    # ===========================
    # åªç”»ä¸€ä¸ªæ•£ç‚¹å›¾ï¼šæ¨ªè½´ä¸ºComputeï¼ˆCï¼‰ï¼Œçºµè½´ä¸ºError Rateï¼Œä¸åŒæ¨¡å‹å¤§å°ç”¨ä¸åŒé¢œè‰²
    # ===========================
    import matplotlib.pyplot as plt

    # åªå¤„ç†ç¬¬ä¸€ä¸ªmetric
    metric_name = TEST_METRICS[0]
    # è®¡ç®—Error Rate
    df['ErrRate'] = 1 - df[metric_name]
    
    # è®¡ç®—Improvement Rateï¼šç›¸å¯¹äºstep=0çš„æ”¹è¿›ç‡
    def calc_improvement_rate_for_group(group):
        step_0_rows = group[group['step'] == 0]
        if len(step_0_rows) == 0:
            raise ValueError(f"No step=0 found for model_size={group['model_size'].iloc[0]}")
        baseline_score = step_0_rows[metric_name].iloc[0]
        group = group.copy()
        group['ImprovementRate'] = group[metric_name] / baseline_score
        return group
    
    df = df.groupby('model_size', group_keys=False).apply(calc_improvement_rate_for_group).reset_index(drop=True)
    
    # è®¡ç®—å®Œ ImprovementRate åï¼Œä¸¢å¼ƒ step=0 çš„æ•°æ®ï¼ˆå› ä¸º E=0 ä¼šå¯¼è‡´ log10(E) = -infï¼‰
    df = df[df['step'] > 0].reset_index(drop=True)
    
    # ä¸¢æ‰æ¯ä¸ª (model_size, runid) çš„å‰ WARMUP_CLIPPING_NUM ä¸ªç‚¹
    if WARMUP_CLIPPING_NUM and WARMUP_CLIPPING_NUM > 0:
        df = (
            df.groupby(['model_size', 'runid'], as_index=False, group_keys=False)
              .apply(lambda g: g.iloc[WARMUP_CLIPPING_NUM:])
              .reset_index(drop=True)
        )
    
    # å¯¹ç›¸åŒæ¨ªåæ ‡ï¼ˆåŒä¸€ model_size ä¸ step â†’ åŒä¸€ Eï¼‰èšåˆï¼šåªæ˜¾ç¤ºä¸‰ä¸ªçºµåæ ‡ï¼ˆä¸åŒ runï¼‰çš„å¹³å‡å€¼
    df_mean = (
        df.groupby(['model_size', 'step'], as_index=False)
          .agg(N=('N', 'first'), C=('C', 'first'), E=('E', 'first'), ErrRate=('ErrRate', 'mean'), ImprovementRate=('ImprovementRate', 'mean'))
    )
    # é¢œè‰²æ˜ å°„
    color_map = {
        0.5e9: '#1f77b4',
        1.5e9: '#ff7f0e',
        3e9: '#d62728',
        7e9: '#2ca02c',
        14e9: '#9467bd',
    }
    # å¦‚æœæœ‰æ›´å¤šæ¨¡å‹å¤§å°ï¼ŒæŒ‰æ•°å€¼ N æ’åºï¼ˆç¡®ä¿å¤§æ¨¡å‹å¦‚ 14B æ’åœ¨æœ€åï¼‰
    model_order = (
        df_mean[['model_size', 'N']]
        .drop_duplicates()
        .sort_values('N')
        ['model_size']
        .tolist()
    )
    unique_model_sizes = model_order
    import itertools
    import matplotlib
    color_cycle = itertools.cycle(matplotlib.colormaps['tab10'].colors)
    for ms in unique_model_sizes:
        if ms not in color_map:
            color_map[ms] = next(color_cycle)

    plt.figure(figsize=(7, 5))
    for ms in unique_model_sizes:
        subdf = df_mean[df_mean['model_size'] == ms]
        plt.scatter(
            subdf['E'], 
            subdf['ErrRate'], 
            # ç›´æ¥ç”¨å°å†™bæ˜¾ç¤ºï¼ˆå¦‚1.5bï¼‰ï¼Œä¸åŒºåˆ†å¤§å°
            label=f"{ms}",
            color=color_map[ms], 
            alpha=0.7, 
            s=12
        )
    plt.xscale('log')
    plt.yscale('log')

    plt.xlabel("Training Examples E (log)")
    plt.ylabel("Error Rate")
    plt.title("Error Rate vs Training Examples")
    plt.legend(title="model size", loc="best")
    plt.tight_layout()
    plt.savefig(OUTPUT_BASE_DIR / f"{FIGURE_PREFIX}_scatter_errrate_vs_E.pdf", dpi=300, bbox_inches='tight')
    print(f"æ•£ç‚¹å›¾å·²ä¿å­˜åˆ° {OUTPUT_BASE_DIR / f'{FIGURE_PREFIX}_scatter_errrate_vs_E.pdf'}")

    # ===========================
    # æ‹Ÿåˆæ¨¡å‹ï¼šå¯¹æ•°çº¿æ€§æ¨¡å‹
    # å˜é‡å‘½åï¼šErrRate=Error Rate, N=model_size, E=Training Examples
    # ===========================
    N_all = df_mean['N'].to_numpy(dtype=float)
    E_all = df_mean['E'].to_numpy(dtype=float)  # Training Examples (x-axis)
    ErrRate_all = df_mean['ErrRate'].to_numpy(dtype=float)  # Error Rate (y-axis)

    # å®‰å…¨å¤„ç†ï¼šé¿å… log(0)
    eps = 1e-12

    # ---------------------------
    # æ¨¡å‹ 0 (åŒLogisticæ‹Ÿåˆç‰ˆ)ï¼šlog10(ErrRate) = -k(N) * log10(E) + E0(N)
    # å…¶ä¸­ k(N) = L / (1 + exp(-r * (N - N0)))ï¼ŒE0(N) = A_e0 / (1 + exp(r_e0 * (N - N0_e0))) + B_e0
    # ---------------------------
    print("\n=== æ¨¡å‹0 åŒLogisticæ‹Ÿåˆç‰ˆï¼šlog10(ErrRate) = -k(N) * log10(E) + E0(N) ===")
    print("å…¶ä¸­ k(N) = L / (1 + exp(-r * (N - N0)))")
    print("E0(N) = A_e0 / (1 + exp(r_e0 * (N - N0_e0))) + B_e0")
    
    # å‡†å¤‡æ‰€æœ‰æ•°æ®
    N_all_data = []
    E_all_data = []
    ErrRate_all_data = []
    
    for ms in unique_model_sizes:
        subdf = df_mean[df_mean['model_size'] == ms]
        N_val = float(subdf['N'].iloc[0])
        E_vals = subdf['E'].to_numpy(dtype=float)
        ErrRate_vals = np.clip(subdf['ErrRate'].to_numpy(dtype=float), 1e-12, None)
        
        N_all_data.extend([N_val] * len(E_vals))
        E_all_data.extend(E_vals)
        ErrRate_all_data.extend(ErrRate_vals)
    
    N_all_data = np.array(N_all_data)
    E_all_data = np.array(E_all_data)
    ErrRate_all_data = np.array(ErrRate_all_data)
    
    # è½¬æ¢ä¸ºå¯¹æ•°ç©ºé—´
    log10_E_all = np.log10(E_all_data)
    log10_ErrRate_all = np.log10(ErrRate_all_data)
    

    # å±€åŸŸå‡½æ•°å®šä¹‰
    def logistic_k(N, L, r, N0_k):
        """Logisticå‡½æ•°ï¼šk(N) = L / (1 + exp(-r * (N - N0_k)))"""
        return L / (1 + np.exp(-r * (N - N0_k)))

    def logistic_e0(N, L, r_e0, N0_e0, B_e0):
        """Logisticå‡½æ•°ç”¨äºE0(N)ï¼šE0(N) = L / (1 + exp(r_e0 * (N - N0_e0))) + B_e0"""
        return L / (1 + np.exp(r_e0 * (N - N0_e0))) + B_e0

    def global_model(params, N, log10_E):
        """åŒLogisticæ‹Ÿåˆå‡½æ•°ï¼šlog10(ErrRate) = -k(N) * log10(E) + E0(N)
        å…¶ä¸­ k(N) = L / (1 + exp(-r * (N - N0_k)))
        E0(N) = L / (1 + exp(r_e0 * (N - N0_e0))) + B_e0
        """
        # å…±äº«å‚æ•°ï¼šL
        # k(N)å‚æ•°ï¼šr, N0_k
        # E0(N)å‚æ•°ï¼šr_e0, N0_e0, B_e0
        L, r, N0_k, r_e0, N0_e0, B_e0 = params

        k = logistic_k(N, L, r, N0_k)
        E0 = logistic_e0(N, L, r_e0, N0_e0, B_e0)

        return -k * log10_E + E0

    # # å®šä¹‰å…¨å±€æ‹Ÿåˆå‡½æ•°ï¼šlog10(ErrRate) = -(a * N + b) * log10(E) + E0
    # def global_model(params, N, log10_E):
    #     a, b, E0 = params
    #     k = a * N + b  # k(N) = a * N + b
    #     return -k * log10_E + E0
    
    # ä½¿ç”¨scipyè¿›è¡Œéçº¿æ€§æ‹Ÿåˆ
    from scipy.optimize import curve_fit
    
    # # åˆå§‹å‚æ•°ä¼°è®¡
    # a_init = 3.65e-12  # åŸºäºä¹‹å‰çš„åˆ†æ
    # b_init = 0.0061    # åŸºäºä¹‹å‰çš„åˆ†æ
    # E0_init = 0.0      # ä»0å¼€å§‹

    # è·å–æ‰€æœ‰å”¯ä¸€çš„Nå€¼
    unique_N_list = sorted([float(subdf['N'].iloc[0]) for ms in unique_model_sizes 
                           for subdf in [df_mean[df_mean['model_size'] == ms]] if len(subdf) > 0])
    n_models = len(unique_N_list)
    
    print(f"å‘ç° {n_models} ä¸ªä¸åŒçš„æ¨¡å‹å¤§å°: {[f'{n:.1e}' for n in unique_N_list]}")
    
    # åˆå§‹å‚æ•°ä¼°è®¡
    # å…±äº«å‚æ•°
    L_init = 0.06      # å…±äº«çš„æœ€å¤§æŒ¯å¹…
    
    # k(N) å‚æ•°
    r_init = 2e-10     # kçš„å¢é•¿ç‡
    N0_k_init = 5e9    # kçš„æ‹ç‚¹ä½ç½®
    
    # E0(N) å‚æ•°
    r_e0_init = 1e-9   # E0çš„å¢é•¿ç‡
    N0_e0_init = 3e9   # E0çš„æ‹ç‚¹ä½ç½®
    B_e0_init = 0  # E0çš„ä¸‹æ¸è¿‘çº¿
    
    # try:
    if True:
        # å®šä¹‰æ‹Ÿåˆå‡½æ•°åŒ…è£…å™¨
        # def fit_func(data, a, b, E0):
        #     N, log10_E = data
        #     return global_model([a, b, E0], N, log10_E)
        def fit_func(data, L, r, N0_k, r_e0, N0_e0, B_e0):
            N, log10_E = data
            params = [L, r, N0_k, r_e0, N0_e0, B_e0]
            return global_model(params, N, log10_E)
        
        # popt, pcov = curve_fit(
        #     fit_func,
        #     (N_all_data, log10_E_all),
        #     log10_ErrRate_all,
        #     p0=[a_init, b_init, E0_init],
        #     maxfev=10000
        # )

        # æ„å»ºåˆå§‹å‚æ•°å’Œè¾¹ç•Œ
        p0 = [L_init, r_init, N0_k_init, r_e0_init, N0_e0_init, B_e0_init]
        # æ›´åˆç†çš„è¾¹ç•Œè®¾ç½®
        lower_bounds = [0, 0, 0, 0, 0, -0.5]  # L>0, r>0, N0_k>0, r_e0>0, N0_e0>0, B_e0>-0.5
        upper_bounds = [1, 1e-6, 1e12, 1e-6, 1e12, 0.5]  # B_e0çš„èŒƒå›´ç¼©å°åˆ°[-0.5, 0.5]
        
        print(f"å‚æ•°æ•°é‡: {len(p0)} (å…±äº«å‚æ•°: L; k(N): r, N0_k; E0(N): r_e0, N0_e0, B_e0)")
        print(f"å‚æ•°è¾¹ç•Œ: L[0,1], r[0,1e-6], N0_k[0,1e12], r_e0[0,1e-6], N0_e0[0,1e12], B_e0[-0.5,0.5]")
        
        # ä½¿ç”¨æ›´ä¸¥æ ¼çš„æ”¶æ•›æ¡ä»¶å’Œå¤šæ¬¡å°è¯•ä¸åŒåˆå§‹å€¼
        best_popt = None
        best_r2 = -np.inf
        best_result = None
        
        # å°è¯•ä¸åŒçš„B_e0åˆå§‹å€¼ä»¥é¿å…å±€éƒ¨æœ€ä¼˜
        B_e0_candidates = [0, -0.1, 0.1, -0.05, 0.05]
        
        for i, B_e0_try in enumerate(B_e0_candidates):
            try:
                p0_try = [L_init, r_init, N0_k_init, r_e0_init, N0_e0_init, B_e0_try]
                print(f"å°è¯• #{i+1}: B_e0_init = {B_e0_try}")
                
                popt_try, pcov_try = curve_fit(
                    fit_func,
                    (N_all_data, log10_E_all),
                    log10_ErrRate_all,
                    p0=p0_try,
                    bounds=(lower_bounds, upper_bounds),
                    maxfev=100000,  # å¢åŠ æœ€å¤§è¿­ä»£æ¬¡æ•°
                    ftol=1e-12,     # è®¾ç½®å‡½æ•°æ”¶æ•›ç²¾åº¦
                    xtol=1e-12,     # è®¾ç½®å‚æ•°æ”¶æ•›ç²¾åº¦
                    gtol=1e-12      # è®¾ç½®æ¢¯åº¦æ”¶æ•›ç²¾åº¦
                )
                
                # è®¡ç®—RÂ²
                y_pred_try = global_model(popt_try, N_all_data, log10_E_all)
                ss_res_try = np.sum((log10_ErrRate_all - y_pred_try) ** 2)
                ss_tot_try = np.sum((log10_ErrRate_all - np.mean(log10_ErrRate_all)) ** 2)
                r2_try = 1.0 - ss_res_try / ss_tot_try if ss_tot_try > 0 else np.nan
                
                print(f"  RÂ² = {r2_try:.6f}")
                
                if r2_try > best_r2:
                    best_r2 = r2_try
                    best_popt = popt_try
                    best_result = (popt_try, pcov_try)
                    
            except Exception as e:
                print(f"  æ‹Ÿåˆå¤±è´¥: {e}")
                continue
        
        if best_popt is None:
            raise RuntimeError("æ‰€æœ‰åˆå§‹å€¼å°è¯•éƒ½å¤±è´¥äº†ï¼Œè¯·æ£€æŸ¥æ•°æ®å’Œå‚æ•°è®¾ç½®")
            
        popt, pcov = best_result
        print(f"\nâœ… æœ€ä½³æ‹Ÿåˆç»“æœ: RÂ² = {best_r2:.6f}")
        
        # æå–æ‹Ÿåˆå‚æ•°
        L_fit, r_fit, N0_k_fit, r_e0_fit, N0_e0_fit, B_e0_fit = popt
        
        # è®¡ç®—å…¨å±€RÂ²
        y_pred_all = global_model(popt, N_all_data, log10_E_all)
        ss_res = np.sum((log10_ErrRate_all - y_pred_all) ** 2)
        ss_tot = np.sum((log10_ErrRate_all - np.mean(log10_ErrRate_all)) ** 2)
        r2_global = 1.0 - ss_res / ss_tot if ss_tot > 0 else np.nan
        
        # print(f"å…¨å±€æ‹Ÿåˆå‚æ•° k(N) = a * N + b:")
        # print(f"a = {a_fit:.2e}")
        # print(f"b = {b_fit:.6f}")
        # print(f"E0 = {E0_fit:.6f}")
        # print(f"k(N) = {a_fit:.2e} * N + {b_fit:.6f}")
        # print(f"log10(ErrRate) = -k(N) * log10(E) + {E0_fit:.6f}")
        # print(f"å…¨å±€ RÂ² = {r2_global:.4f}")
        
        print(f"\nâœ… åŒLogisticæ‹ŸåˆæˆåŠŸï¼")
        print(f"\nå…±äº«å‚æ•°:")
        print(f"  L = {L_fit:.6f} (å…±äº«æœ€å¤§æŒ¯å¹…)")
        
        print(f"\nk(N) å‚æ•°:")
        print(f"  r = {r_fit:.2e} (kçš„å¢é•¿ç‡)")
        print(f"  N0_k = {N0_k_fit:.2e} (kçš„æ‹ç‚¹ä½ç½®)")
        print(f"  k(N) = {L_fit:.6f} / (1 + exp(-{r_fit:.2e} * (N - {N0_k_fit:.2e})))")
        
        print(f"\nE0(N) å‚æ•°:")
        print(f"  r_e0 = {r_e0_fit:.2e} (E0çš„å¢é•¿ç‡)")
        print(f"  N0_e0 = {N0_e0_fit:.2e} (E0çš„æ‹ç‚¹ä½ç½®)")
        print(f"  B_e0 = {B_e0_fit:.6f} (E0çš„ä¸‹æ¸è¿‘çº¿)")
        print(f"  E0(N) = {L_fit:.6f} / (1 + exp({r_e0_fit:.2e} * (N - {N0_e0_fit:.2e}))) + {B_e0_fit:.6f}")
        
        print(f"\nå…¨å±€ RÂ² = {r2_global:.4f}")
        
        # è®¡ç®—å¹¶æ˜¾ç¤ºæ¯ä¸ªæ¨¡å‹å¤§å°å¯¹åº”çš„E0å€¼
        print(f"\næ¯ä¸ªæ¨¡å‹å¤§å°å¯¹åº”çš„E0(N)é¢„æµ‹å€¼:")
        E0_fit_list = [logistic_e0(n_val, L_fit, r_e0_fit, N0_e0_fit, B_e0_fit) for n_val in unique_N_list]
        for i, n_val in enumerate(unique_N_list):
            print(f"  N = {n_val:.2e}: E0(N) = {E0_fit_list[i]:.6f}")
        
        # ===========================
        # ç»˜åˆ¶E0 vs Nçš„scatterå›¾
        # ===========================
        plt.figure(figsize=(10, 6))
        
        # åˆ›å»ºå­å›¾
        plt.subplot(1, 2, 1)
        # E0 vs N scatterå›¾ - å¯¹æ•°å°ºåº¦
        plt.scatter(unique_N_list, E0_fit_list, color='red', s=80, alpha=0.8, 
                   marker='o', edgecolor='black', linewidth=1, zorder=5)
        
        # æ·»åŠ æ•°å€¼æ ‡ç­¾
        for i, (n_val, e0_val) in enumerate(zip(unique_N_list, E0_fit_list)):
            plt.annotate(f'{e0_val:.3f}', 
                        (n_val, e0_val), 
                        textcoords="offset points", 
                        xytext=(0,10), 
                        ha='center', 
                        fontsize=9)
        
        plt.xscale('log')
        plt.xlabel('Model Size N (parameters)')
        plt.ylabel('E0 parameter')
        plt.title('E0 vs Model Size N (Log Scale)')
        plt.grid(True, alpha=0.3)
        
        # çº¿æ€§å°ºåº¦çš„E0 vs Nå›¾
        plt.subplot(1, 2, 2)
        plt.scatter([n/1e9 for n in unique_N_list], E0_fit_list, color='blue', s=80, alpha=0.8,
                   marker='s', edgecolor='black', linewidth=1, zorder=5)
        
        # æ·»åŠ æ•°å€¼æ ‡ç­¾
        for i, (n_val, e0_val) in enumerate(zip(unique_N_list, E0_fit_list)):
            plt.annotate(f'{e0_val:.3f}', 
                        (n_val/1e9, e0_val), 
                        textcoords="offset points", 
                        xytext=(0,10), 
                        ha='center', 
                        fontsize=9)
        
        plt.xlabel('Model Size N (Billions of parameters)')
        plt.ylabel('E0 parameter')
        plt.title('E0 vs Model Size N (Linear Scale)')
        plt.grid(True, alpha=0.3)
        
        plt.tight_layout()
        out_path_e0_scatter = OUTPUT_BASE_DIR / f"{FIGURE_PREFIX}_E0_vs_N_scatter.pdf"
        plt.savefig(out_path_e0_scatter, dpi=300, bbox_inches='tight')
        print(f"\nE0 vs N scatterå›¾å·²ä¿å­˜åˆ° {out_path_e0_scatter}")
        
        # åˆ†æE0(N)çš„è¶‹åŠ¿
        print(f"\n=== E0(N)å…³ç³»åˆ†æ ====")
        
        # è®¡ç®—E0çš„ç»Ÿè®¡ä¿¡æ¯
        E0_min = min(E0_fit_list)
        E0_max = max(E0_fit_list)
        E0_mean = np.mean(E0_fit_list)
        E0_std = np.std(E0_fit_list)
        
        print(f"E0èŒƒå›´: [{E0_min:.6f}, {E0_max:.6f}]")
        print(f"E0å‡å€¼: {E0_mean:.6f} Â± {E0_std:.6f}")
        
        # ç®€å•æ‹ŸåˆE0ä¸Nçš„å…³ç³»
        # å°è¯•çº¿æ€§å…³ç³»: E0 = a_e0 * N + b_e0
        try:
            coeffs_linear = np.polyfit(unique_N_list, E0_fit_list, 1)
            a_e0, b_e0 = coeffs_linear
            E0_pred_linear = a_e0 * np.array(unique_N_list) + b_e0
            r2_e0_linear = 1 - np.sum((np.array(E0_fit_list) - E0_pred_linear)**2) / np.sum((np.array(E0_fit_list) - E0_mean)**2)
            print(f"\nçº¿æ€§æ‹Ÿåˆ E0(N) = a*N + b:")
            print(f"  a = {a_e0:.2e}")
            print(f"  b = {b_e0:.6f}")
            print(f"  RÂ² = {r2_e0_linear:.4f}")
        except:
            print("çº¿æ€§æ‹Ÿåˆå¤±è´¥")
        
        # å°è¯•å¯¹æ•°å…³ç³»: E0 = a_e0 * log(N) + b_e0
        try:
            coeffs_log = np.polyfit(np.log(unique_N_list), E0_fit_list, 1)
            a_e0_log, b_e0_log = coeffs_log
            E0_pred_log = a_e0_log * np.log(unique_N_list) + b_e0_log
            r2_e0_log = 1 - np.sum((np.array(E0_fit_list) - E0_pred_log)**2) / np.sum((np.array(E0_fit_list) - E0_mean)**2)
            print(f"\nå¯¹æ•°æ‹Ÿåˆ E0(N) = a*log(N) + b:")
            print(f"  a = {a_e0_log:.6f}")
            print(f"  b = {b_e0_log:.6f}")
            print(f"  RÂ² = {r2_e0_log:.4f}")
        except:
            print("å¯¹æ•°æ‹Ÿåˆå¤±è´¥")
        
        # å°è¯•Logisticå‡½æ•°æ‹Ÿåˆ E0(N)
        # å¯¹äºé€’å‡çš„è¶‹åŠ¿ï¼Œä½¿ç”¨åå‘Logistic: E0(N) = L_e0 - A_e0 / (1 + exp(-r_e0 * (N - N0_e0)))
        # æˆ–è€…ç®€åŒ–ä¸º: E0(N) = A_e0 / (1 + exp(r_e0 * (N - N0_e0))) + B_e0
        def logistic_e0_func(N, A_e0, r_e0, N0_e0, B_e0):
            """
            Logisticå‡½æ•°æ‹ŸåˆE0(N)
            E0(N) = A_e0 / (1 + exp(r_e0 * (N - N0_e0))) + B_e0
            """
            return A_e0 / (1 + np.exp(r_e0 * (N - N0_e0))) + B_e0
        
        try:
            # åˆå§‹å‚æ•°ä¼°è®¡
            A_e0_init = max(E0_fit_list) - min(E0_fit_list)  # æŒ¯å¹…
            r_e0_init = 1e-9  # å¢é•¿ç‡ï¼ˆæ­£å€¼è¡¨ç¤ºé€’å‡ï¼‰
            N0_e0_init = np.median(unique_N_list)  # æ‹ç‚¹
            B_e0_init = min(E0_fit_list)  # ä¸‹æ¸è¿‘çº¿
            
            print(f"\nLogisticæ‹Ÿåˆ E0(N) = A/(1 + exp(r*(N-N0))) + B:")
            print(f"åˆå§‹å‚æ•°ä¼°è®¡: A={A_e0_init:.6f}, r={r_e0_init:.2e}, N0={N0_e0_init:.2e}, B={B_e0_init:.6f}")
            
            popt_e0_logistic, pcov_e0_logistic = curve_fit(
                logistic_e0_func, 
                unique_N_list, 
                E0_fit_list,
                p0=[A_e0_init, r_e0_init, N0_e0_init, B_e0_init],
                bounds=([0, 0, 0, -0.5], [1, 1e-6, 1e12, 0.5]),  # åŒæ ·é™åˆ¶Bçš„èŒƒå›´
                maxfev=50000,   # å¢åŠ è¿­ä»£æ¬¡æ•°
                ftol=1e-12,     # è®¾ç½®æ”¶æ•›ç²¾åº¦
                xtol=1e-12,
                gtol=1e-12
            )
            
            A_e0_fit, r_e0_fit, N0_e0_fit, B_e0_fit = popt_e0_logistic
            E0_pred_logistic = logistic_e0_func(unique_N_list, *popt_e0_logistic)
            r2_e0_logistic = 1 - np.sum((np.array(E0_fit_list) - E0_pred_logistic)**2) / np.sum((np.array(E0_fit_list) - E0_mean)**2)
            
            print(f"  A = {A_e0_fit:.6f} (æŒ¯å¹…)")
            print(f"  r = {r_e0_fit:.2e} (å¢é•¿ç‡)")
            print(f"  N0 = {N0_e0_fit:.2e} (æ‹ç‚¹ä½ç½®)")
            print(f"  B = {B_e0_fit:.6f} (ä¸‹æ¸è¿‘çº¿)")
            print(f"  RÂ² = {r2_e0_logistic:.4f}")
            
            # åœ¨E0 vs Nå›¾ä¸­æ·»åŠ Logisticæ‹Ÿåˆçº¿
            # é‡æ–°ç»˜åˆ¶E0 vs Nå›¾ï¼ŒåŠ ä¸ŠLogisticæ‹Ÿåˆçº¿
            plt.figure(figsize=(12, 8))
            
            # å­å›¾1: å¯¹æ•°å°ºåº¦ + æ‰€æœ‰æ‹Ÿåˆçº¿
            plt.subplot(2, 2, 1)
            plt.scatter(unique_N_list, E0_fit_list, color='red', s=80, alpha=0.8, 
                       marker='o', edgecolor='black', linewidth=1, zorder=5, label='Actual E0 values')
            
            # ç»˜åˆ¶æ‹Ÿåˆçº¿
            N_smooth = np.logspace(np.log10(min(unique_N_list)), np.log10(max(unique_N_list)), 200)
            
            # Logisticæ‹Ÿåˆçº¿
            E0_smooth_logistic = logistic_e0_func(N_smooth, *popt_e0_logistic)
            plt.plot(N_smooth, E0_smooth_logistic, 'b-', linewidth=2, label=f'Logistic (RÂ²={r2_e0_logistic:.3f})')
            
            # çº¿æ€§æ‹Ÿåˆçº¿
            if 'r2_e0_linear' in locals():
                E0_smooth_linear = a_e0 * N_smooth + b_e0
                plt.plot(N_smooth, E0_smooth_linear, 'g--', linewidth=2, label=f'Linear (RÂ²={r2_e0_linear:.3f})')
            
            # å¯¹æ•°æ‹Ÿåˆçº¿
            if 'r2_e0_log' in locals():
                E0_smooth_log = a_e0_log * np.log(N_smooth) + b_e0_log
                plt.plot(N_smooth, E0_smooth_log, 'm:', linewidth=2, label=f'Logarithmic (RÂ²={r2_e0_log:.3f})')
            
            plt.xscale('log')
            plt.xlabel('Model Size N (parameters)')
            plt.ylabel('E0 parameter')
            plt.title('E0(N) Fitting Comparison - Log Scale')
            plt.legend()
            plt.grid(True, alpha=0.3)
            
            # å­å›¾2: çº¿æ€§å°ºåº¦
            plt.subplot(2, 2, 2)
            plt.scatter([n/1e9 for n in unique_N_list], E0_fit_list, color='blue', s=80, alpha=0.8,
                       marker='s', edgecolor='black', linewidth=1, zorder=5, label='Actual E0 values')
            
            N_smooth_linear = np.linspace(min(unique_N_list), max(unique_N_list), 200)
            
            # Logisticæ‹Ÿåˆçº¿
            E0_smooth_logistic_linear = logistic_e0_func(N_smooth_linear, *popt_e0_logistic)
            plt.plot([n/1e9 for n in N_smooth_linear], E0_smooth_logistic_linear, 'b-', linewidth=2, label=f'Logistic (RÂ²={r2_e0_logistic:.3f})')
            
            plt.xlabel('Model Size N (Billions of parameters)')
            plt.ylabel('E0 parameter')
            plt.title('E0(N) Fitting Comparison - Linear Scale')
            plt.legend()
            plt.grid(True, alpha=0.3)
            
            # å­å›¾3: æ®‹å·®åˆ†æ
            plt.subplot(2, 2, 3)
            residuals_logistic = np.array(E0_fit_list) - E0_pred_logistic
            plt.scatter(unique_N_list, residuals_logistic, color='red', alpha=0.8)
            plt.axhline(y=0, color='black', linestyle='--')
            plt.xscale('log')
            plt.xlabel('Model Size N (parameters)')
            plt.ylabel('Logistic Fit Residuals')
            plt.title('Logistic Fit Residual Analysis')
            plt.grid(True, alpha=0.3)
            
            # å­å›¾4: æ‹Ÿåˆè´¨é‡æ¯”è¾ƒ
            plt.subplot(2, 2, 4)
            methods = []
            r2_values = []
            
            if 'r2_e0_linear' in locals():
                methods.append('Linear')
                r2_values.append(r2_e0_linear)
            if 'r2_e0_log' in locals():
                methods.append('Logarithmic')
                r2_values.append(r2_e0_log)
            methods.append('Logistic')
            r2_values.append(r2_e0_logistic)
            
            bars = plt.bar(methods, r2_values, color=['green', 'magenta', 'blue'][:len(methods)], alpha=0.7)
            plt.ylabel('RÂ² Value')
            plt.title('E0(N) Fitting Quality Comparison')
            plt.ylim(0, 1)
            
            # åœ¨æŸ±çŠ¶å›¾ä¸Šæ·»åŠ æ•°å€¼æ ‡ç­¾
            for bar, r2 in zip(bars, r2_values):
                height = bar.get_height()
                plt.text(bar.get_x() + bar.get_width()/2., height + 0.01,
                        f'{r2:.3f}', ha='center', va='bottom')
            
            plt.grid(True, alpha=0.3)
            
            plt.tight_layout()
            out_path_e0_analysis = OUTPUT_BASE_DIR / f"{FIGURE_PREFIX}_E0_analysis_comprehensive.pdf"
            plt.savefig(out_path_e0_analysis, dpi=300, bbox_inches='tight')
            print(f"\nE0(N)ç»¼åˆåˆ†æå›¾å·²ä¿å­˜åˆ° {out_path_e0_analysis}")
            
            # æ¯”è¾ƒä¸åŒæ‹Ÿåˆæ–¹æ³•çš„è´¨é‡
            print(f"\n=== E0(N)æ‹Ÿåˆè´¨é‡æ¯”è¾ƒ ====")
            if 'r2_e0_linear' in locals():
                print(f"çº¿æ€§æ‹Ÿåˆ RÂ²: {r2_e0_linear:.4f}")
            if 'r2_e0_log' in locals():
                print(f"å¯¹æ•°æ‹Ÿåˆ RÂ²: {r2_e0_log:.4f}")
            print(f"Logisticæ‹Ÿåˆ RÂ²: {r2_e0_logistic:.4f}")
            
            best_r2 = max([r2 for r2 in [r2_e0_linear if 'r2_e0_linear' in locals() else 0, 
                          r2_e0_log if 'r2_e0_log' in locals() else 0, 
                          r2_e0_logistic]])
            if best_r2 == r2_e0_logistic:
                print(f"\nğŸ† Logisticå‡½æ•°æ‹Ÿåˆæ•ˆæœæœ€ä½³ï¼")
                print(f"E0(N) = {A_e0_fit:.6f} / (1 + exp({r_e0_fit:.2e} * (N - {N0_e0_fit:.2e}))) + {B_e0_fit:.6f}")
            
        except Exception as e:
            print(f"Logisticæ‹Ÿåˆå¤±è´¥: {e}")
            
        # ç»˜å›¾
        model0_stats = []
        plt.figure(figsize=(7, 5))
        
        # å…ˆç”»æ•£ç‚¹
        for ms in unique_model_sizes:
            subdf = df_mean[df_mean['model_size'] == ms].sort_values('E')
            E_vals = subdf['E'].to_numpy(dtype=float)
            ErrRate_vals = np.clip(subdf['ErrRate'].to_numpy(dtype=float), 1e-12, None)
            x = np.log10(E_vals)
            y = np.log10(ErrRate_vals)
            y0 = y[0]  # èµ·å§‹ç‚¹ï¼Œç”¨äºç›¸å¯¹æ˜¾ç¤º
            plt.scatter(
                # x, y - y0, label=f"{ms}",
                x, y, label=f"{ms}",
                color=color_map[ms], alpha=0.6, s=12
            )
        
        # ç”»æ‹Ÿåˆçº¿
        for ms in unique_model_sizes:
            subdf = df_mean[df_mean['model_size'] == ms]
            if len(subdf) < 2:
                continue
            E_vals = subdf['E'].to_numpy(dtype=float)
            ErrRate_vals = np.clip(subdf['ErrRate'].to_numpy(dtype=float), 1e-12, None)
            x = np.log10(E_vals)
            y = np.log10(ErrRate_vals)
            y0 = y[0]
            
            N_val = float(subdf['N'].iloc[0])
            k_logistic = logistic_k(N_val, L_fit, r_fit, N0_k_fit)  # ä½¿ç”¨Logisticæ‹Ÿåˆçš„k(N)å…³ç³»
            
            # è®¡ç®—E0å€¼ï¼ˆä½¿ç”¨Logisticå‡½æ•°ï¼‰
            E0_local = logistic_e0(N_val, L_fit, r_e0_fit, N0_e0_fit, B_e0_fit)
            
            # è®¡ç®—è¯¥æ¨¡å‹å¤§å°çš„RÂ²
            y_pred = global_model(popt, np.array([N_val] * len(x)), x)
            ss_res = np.sum((y - y_pred) ** 2)
            ss_tot = np.sum((y - np.mean(y)) ** 2)
            r2_local = 1.0 - ss_res / ss_tot if ss_tot > 0 else np.nan
            
            model0_stats.append({
                'N': N_val, 'k': k_logistic, 'E0': E0_local, 'r2_log': r2_local,
                'E_min': E_vals.min(), 'E_max': E_vals.max(), 'y0': y0
            })
            print(f"N={N_val:.3g}: k={k_logistic:.6f} (Logistic), E0={E0_local:.6f} (Logistic), R2(log10 ErrRate)={r2_local:.4f}")
            
            # ç”»æ‹Ÿåˆçº¿
            E_grid = np.logspace(np.log10(E_vals.min()), np.log10(E_vals.max()), 200)
            x_grid = np.log10(E_grid)
            y_grid = global_model(popt, np.array([N_val] * len(x_grid)), x_grid)
            # plt.plot(x_grid, y_grid - y0, color=color_map[ms], linewidth=2, linestyle='--')
            plt.plot(x_grid, y_grid, color=color_map[ms], linewidth=2, linestyle='--')
        
    # except Exception as e:
    #     print(f"å…¨å±€æ‹Ÿåˆå¤±è´¥: {e}")
    #     model0_stats = []

    # çº¿æ€§åæ ‡ï¼šx=log10(E), y=Î”log10(ErrRate)
    plt.xlabel(r"$\log_{10}E$")
    plt.ylabel(r"$\log_{10}ErrRate$")
    
    # æ›´æ–°æ ‡é¢˜æ˜¾ç¤ºå®Œæ•´çš„åŒ Logistic å…¬å¼ï¼ˆæŠ½è±¡å½¢å¼ï¼‰
    title_text = (r"Dual-Logistic Model: $\log_{10}ErrRate = -k(N) \cdot \log_{10}E + E_0(N)$" + "\n" +
                 r"$k(N) = \frac{L}{1 + \exp(-r \cdot (N - N_{0k}))}$" + "\n" +
                 r"$E_0(N) = \frac{L}{1 + \exp(r_{e0} \cdot (N - N_{0e0}))} + B$")
    plt.title(title_text, fontsize=11, pad=20)
    
    # åœ¨å›¾ä¸Šæ·»åŠ æ‹Ÿåˆå‚æ•°ä¿¡æ¯
    info_text = (
        f"Fitting Results:\n"
        f"Global RÂ² = {r2_global:.4f}\n"
        f"Shared Parameter:\n"
        f"  L = {L_fit:.6f}\n"
        f"k(N) Parameters:\n"
        f"  r = {r_fit:.2e}\n"
        f"  Nâ‚€k = {N0_k_fit:.2e}\n"
        f"Eâ‚€(N) Parameters:\n"
        f"  r_e0 = {r_e0_fit:.2e}\n"
        f"  Nâ‚€e0 = {N0_e0_fit:.2e}\n"
        f"  B = {B_e0_fit:.6f}"
    )
    
    # åœ¨å›¾çš„å³ä¸Šè§’æ·»åŠ ä¿¡æ¯æ¡†
    plt.text(0.98, 0.98, info_text, transform=plt.gca().transAxes, 
             fontsize=8, verticalalignment='top', horizontalalignment='right',
             bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8))
    
    plt.legend(title="Model Size", loc="lower left")
    plt.tight_layout()
    out_path0 = OUTPUT_BASE_DIR / f"{FIGURE_PREFIX}_fit_model0.pdf"
    plt.savefig(out_path0, dpi=300, bbox_inches='tight')
    print(f"æ‹Ÿåˆæ›²çº¿å›¾å·²ä¿å­˜åˆ° {out_path0}")

    print(f"\n=== ErrRate æ¨¡å‹æ‹Ÿåˆå®Œæˆ ===")
    print("æ¨¡å‹ï¼šåŒLogisticæ‹Ÿåˆ log10(ErrRate) = -k(N) * log10(E) + E0(N)")
    print(f"å…¶ä¸­ k(N) = {L_fit:.6f} / (1 + exp(-{r_fit:.2e} * (N - {N0_k_fit:.2e})))")
    print(f"E0(N) = {L_fit:.6f} / (1 + exp({r_e0_fit:.2e} * (N - {N0_e0_fit:.2e}))) + {B_e0_fit:.6f}")
    if model0_stats:
        k_mean = np.mean([s['k'] for s in model0_stats])
        r2_mean = np.mean([s['r2_log'] for s in model0_stats])
        print(f"å¹³å‡ k â‰ˆ {k_mean:.3f}")
        print(f"å¹³å‡ R2(log10 ErrRate) â‰ˆ {r2_mean:.3f}")

if __name__ == "__main__":
    main()