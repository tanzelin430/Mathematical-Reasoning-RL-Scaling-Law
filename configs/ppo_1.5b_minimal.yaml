# PPO configuration for minimal test
hydra:
  searchpath:
    - file://verl/verl/trainer/config

defaults:
  - ppo_trainer
  - _self_

# Custom reward function for 4 domains
custom_reward_function:
  path: /fs-computility/mabasic/tanzelin.p/work/Agentic-RL-Scaling-Law/src/reward/guru_reward.py
  name: compute_score

# Data configuration - minimal for testing
data:
  train_files: []  # Will be overridden by command line
  val_files: []    # Will be overridden by command line
  max_prompt_length: 1024
  max_response_length: 2048  # Increased to allow full reasoning
  train_batch_size: 2
  val_batch_size: 2
  filter_overlong_prompts: True
  truncation: 'error'
  dataloader_num_workers: 2
  shuffle: True
  seed: 42

# Model configuration for 1.5B model
actor_rollout_ref:
  model:
    path: /fs-computility/mabasic/shared/models/DeepSeek-R1-Distill-Qwen-1.5B
    enable_gradient_checkpointing: True
    trust_remote_code: True
    
  # Actor configuration - only use per_gpu versions
  actor:
    optim:
      lr: 1e-6
    ppo_micro_batch_size_per_gpu: 1
    ppo_mini_batch_size: 2  # Must be <= train_batch_size
    use_kl_loss: True
    kl_loss_coef: 0.01
    entropy_coeff: 0.01
    
  # Rollout configuration  
  rollout:
    name: vllm
    gpu_memory_utilization: 0.3
    tensor_model_parallel_size: 2
    temperature: 0.7
    top_p: 0.9
    n: 1
    log_prob_micro_batch_size_per_gpu: 1
    
  # Reference model - only use per_gpu version
  ref:
    log_prob_micro_batch_size_per_gpu: 1
    fsdp_config:
      param_offload: True

# Critic configuration - only use per_gpu version
critic:
  optim:
    lr: 5e-6
  model:
    path: /fs-computility/mabasic/shared/models/DeepSeek-R1-Distill-Qwen-1.5B
  ppo_micro_batch_size_per_gpu: 1

# Algorithm configuration
algorithm:
  gamma: 1.0
  lam: 0.95
  adv_estimator: gae
  use_kl_in_reward: False
  
# Training configuration
trainer:
  total_epochs: 1
  save_freq: 1
  test_freq: 1
  n_gpus_per_node: 2
  nnodes: 1
  project_name: 'agentic_rl_scaling_law'
  experiment_name: 'ppo_minimal_test'
  logger: ['console']
  critic_warmup: 0
  default_local_dir: results/checkpoints/${trainer.project_name}/${trainer.experiment_name}