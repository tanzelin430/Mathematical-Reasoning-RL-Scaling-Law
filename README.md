# Agentic-RL-Scaling-Law
Agentic RL Scaling Law Experiments

## 实验设置

### 模型
我们的实验需要囊括各个参数量的模型，可从以下具有深度推理能力的基座模型中选择：
- DeepSeek-R1-Distill-Qwen系列：1.5B、7B、14B
- QwQ-32B

### 数据集
数据集：Guru-RL-92k，该数据集是一个混合数据集，本研究用该实验作为初始实验数据集有两大原因，一是保证各分工组的实验数据集和Code-Base保持一致，便于项目推进和讨论；二是在Scaling Law探究的初期可以尽量做出一个General的结论，而避免谈论具体的Domain。在实验后期，我们会做General Scaling Law->Domain-Specific Scaling Law的对比与迁移。同时由于该数据集不含SFT数据，相关数据若需使用需要人工合成。

### Metric
- Pass@1 平均性能提升（横坐标训练步数，纵坐标Pass@1）
- Pass@k曲线（横坐标k，纵坐标Pass@k，在不同的训练步数下放置多张图）
  - 将k=128单独列出来，形成一个“创造性指数”，我们认为在这个k值下面模型已经sample出了它能找到的所有解
  - 该指标用于评测模型是否过拟合到了奖励信号上
  - k_max = 128
- 每单位计算量的性能提升：计算量增加的Flops与Pass@1的上升之间的关系

### 探究点
在实验中，我们遵循控制变量法这一基础原则，我们主要有三大主要变量：模型规模、数据量、训练步数
重点关注前三点
**模型规模与训练步数(N)**【Zelin、chenzhang、zaibin(literature review)】
- 随着参数量增加，使用相同Setting进行RFT（Reinforcement Learning Fine-tuning）之后模型性能是否提升（Pass@1）
- 参数量对于模型稳定性影响：RFT之后，观察模型是否过拟合到奖励模型（观察Pass@K）
- 参数量对于样本效率的影响：随着参数量提升，是否需要更多的训练步数才能让模型收敛（或者说，达到最大性能）
- 更大的模型规模是否可以带来更加稳定的训练过程
- 训练步数与过拟合到奖励信号、以及泛化性之间的关系：在训练过程中如果需要保持模型的泛化性能，是否需要early stop（观察Pass@k与训练步数之间的关系）
**数据规模与比例(D)**【周恒、钰涛】
- RFT过程中的样本数量记为$$D_{RL}$$，对于相同的数据集，更多的$$D_{RL}$$是否可以持续提升模型性能上限，性能上限何时出现（Pass@1），是否存在边际收益递减的情况？
- SFT数据与RFT数据的“规模匹配”问题：通过调整$$
D_{SFT}:D_{RL}$$，观察RFT过程中：1.模型性能的变化Scaling Law是否与上一点中提到的一致（比如，收敛到性能上限的速度，观察曲线斜率，SFT 性能随数据规模的提升是否会使 RL 微调的缩放斜率更陡峭）2. 对于性能上限的影响（Pass@1）
**混合数据训练**【Xiangyuan Yifan】
- 对数据进行难度分级，观察由浅入深地分级进行训练和混合不同难度数据进行训练的收敛曲线是否一致（课程学习or直接开训）
- 对于混合数据的跨领域训练，观察在新领域能够实现Domain-transfer的最小数据量要求，或者说能否有效地实现Domain Transfer。
- 若Domain-Transfer是有效的，则需要探究：
  - 是否越大参数量的模型，在新领域上强化学习效率更高，而参数量少的模型反而难以泛化【待观察实际效果】
  
**奖励模型（RM）的规模与质量**【Preference Based Only，想法待完善】
- RM 的规模与 RL 微调性能的关系：相同数据微调出的更大的 RM 是否能更精准地引导 RL，使主模型性能随 RM规模提升而提升？
- RM 的数据规模（$$D_{RM}$$）：当$$D_{RM}$$增加时，RL 微调的性能是否遵循特定规律？是否存在 “RM 数据瓶颈”（即 RM 的数据不足会限制主模型的缩放潜力）？

**奖励规则的复杂度**【Rule Based Only，想法待完善】
- 规则复杂度与模型规模的交互：更大规模的模型是否更能理解规则的深层逻辑？此时 scaling law 的斜率是否更陡峭（即模型规模对性能的边际效益更高）
- 与 Preferenced RM 的对比：相同模型规模下，rule-based RM 的 scaling law 是否比 learned RM（基于数据训练的 RM）更早出现 “平台期”？
  
**Long Horizon Task多轮交互次数**【想法待完善】
依赖的数据集：https://github.com/abdulhaim/LMRL-Gym
